{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQKEhoBCQoOInU/yGxtBK2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a project on domain name suggestion.\n",
        "\n",
        "Proposing a suitable Domain Name is a tricky assignment for entrepreneurs. Clarity, Pronunciation, Popular Reception, Cultural Implications, trademark laws and regulations shall be taken into account.  \n",
        "\n",
        "Targets include:\n",
        "1. Reproducible Performance with Model Version Tracking\n",
        "2. Runnable evaluation framework that works across all model iterations\n",
        "3. **Optional**: Deploy selected model as API endpoint\n"
      ],
      "metadata": {
        "id": "l8D4uJg2OWPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Step: Create a small, diverse synthetic dataset for domain-name suggestion tasks\n",
        "\n",
        "It includes:\n",
        "1) Briefs (JSONL)\n",
        "\n",
        "2) Candidates with labels (JSONL)\n",
        "\n",
        "3) Pairwise preference judgments (JSONL)\n",
        "\n",
        "4) A README-style methodology (Markdown)\n",
        "\n",
        "\n",
        "# Synthetic Dataset for Domain Name Suggestions\n",
        "\n",
        "**Generated:** {datetime.utcnow().isoformat()}Z\n",
        "\n",
        "## Files\n",
        "- `domain_briefs.jsonl` — Diverse briefs (industry, tone, keywords, constraints, complexity)\n",
        "- `domain_candidates.jsonl` — Candidate suggestions with scores, pass/fail, safety flags\n",
        "- `domain_pairwise.jsonl` — Synthetic pairwise preferences for DPO/IPO\n",
        "\n",
        "## Diversity Coverage\n",
        "- **Business types**: fintech, eco cosmetics, B2B AI, coffee roaster, tutoring (FR), dev tools (DE), travel (ES), wellness, home IoT, climate nonprofit, JP stationery (translit), AR food delivery (translit), pet supplements, outdoor rentals, kids coding.\n",
        "- **Languages/scripts**: EN, FR, DE, ES (Latin). JP/AR represented via **Latin transliteration** to avoid IDN in this first version.\n",
        "- **Complexity levels**: basic, moderate, advanced (randomly assigned) indicating constraints richness and prompt realism.\n",
        "\n",
        "## Methodology\n",
        "1. **Brief Construction**: For each business type, we define language, tone, keywords, and constraints:\n",
        "   - `max_len` (10/12/14), `allowed_tlds` (domain-appropriate),\n",
        "   - forbid digits/hyphens, ASCII-only for v1 (IDN can be added later).\n",
        "2. **Candidate Generation**: Nonce-word generator from a curated syllable bank creates pronounceable, brandable strings. We avoid real brands or adult/illegal terms.\n",
        "3. **Safety & Constraints**: We inject a small fraction of *intentionally flawed* candidates (digits, hyphens, or trademark-like typosquats such as `go0gle-...`) to train and evaluate filters. No explicit harmful content is included.\n",
        "4. **Weak Labels**: Each candidate has pseudo-scores (`brandability`, `brevity`, `keyword_fit`) for reranking studies. Replace with human ratings over time.\n",
        "5. **Pairwise Preferences**: Winners are sampled from higher-scored, constraint-passing candidates; losers from lower-scored/flagged ones. This supports preference optimization (DPO/IPO/KTO).\n",
        "6. **Intended Use**: Bootstrapping a generation→filter→rerank pipeline and automated tests. For production, add multilingual scripts, IDN/homograph checks, and human review.\n",
        "7. **Ethics & Safety**: The dataset purposefully avoids generating or normalizing harmful categories (hate, sexual content, illegal goods/services, self-harm, extremist content). It includes negative examples only in the form of benign constraint violations and obvious trademark-like typosquats to test refusal/filters.\n",
        "\n",
        "## Schemas\n",
        "### Brief\n",
        "```json\n",
        "{{\"brief_id\":\"uuid\",\"title\":\"string\",\"language\":\"en\",\"script\":\"Latin\",\"tone\":\"string\",\"keywords\":[\"k1\",\"k2\"],\"constraints\":{{\"max_len\":12,\"allowed_tlds\":[\".com\",\".io\"],\"forbid_digits\":true,\"forbid_hyphens\":true,\"ascii_only\":true}},\"complexity\":\"basic|moderate|advanced\",\"notes\":\"string\"}}\n",
        "```\n",
        "\n",
        "\n",
        "### Candidate\n",
        "```json\n",
        "{{\"candidate_id\":\"uuid\",\"brief_id\":\"uuid\",\"domain\":\"brevexa.com\",\"rationale\":\"string\",\"scores\":{{\"brandability\":0.85,\"brevity\":0.9,\"keyword_fit\":0.7}},\"passes_constraints\":true,\"safety\":{{\"flagged\":false,\"reasons\":[]}}}}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGlOZ52Ex5ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, uuid, textwrap, os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "HAvnHhEYyaku"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json, random, uuid, os, re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def uid():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "OUT_DIR = \".\"\n",
        "ensure_dir(OUT_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Briefs\n",
        "# -------------------------\n",
        "business_catalog = [\n",
        "    # (title, keywords, tone, tlds, language, script, notes)\n",
        "    (\"Fintech payments wallet\", [\"pay\", \"wallet\", \"secure\"], \"premium, trustworthy\", [\".com\",\".io\",\".pay\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Eco-friendly cosmetics\", [\"vegan\", \"plant\", \"glow\"], \"gentle, natural\", [\".com\",\".beauty\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"B2B AI analytics\", [\"insight\", \"metrics\", \"predict\"], \"modern, technical\", [\".ai\",\".io\",\".com\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Artisanal coffee roaster\", [\"beans\",\"roast\",\"origin\"], \"craft, warm\", [\".com\",\".coffee\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Online French tutoring\", [\"cours\",\"langue\",\"coach\"], \"convivial, sérieux\", [\".fr\",\".com\"], \"fr\", \"Latin\", \"\"),\n",
        "    (\"SaaS developer tools (DE)\", [\"code\",\"build\",\"deploy\"], \"prägnant, professionell\", [\".de\",\".dev\",\".io\"], \"de\", \"Latin\", \"\"),\n",
        "    (\"Travel planning app (ES)\", [\"viaje\",\"ruta\",\"plan\"], \"amable, inspirador\", [\".es\",\".app\",\".com\"], \"es\", \"Latin\", \"\"),\n",
        "    (\"Wellness & yoga studio\", [\"flow\",\"breathe\",\"calm\"], \"soothing, minimalist\", [\".com\",\".studio\",\".fit\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Home automation IoT\", [\"smart\",\"home\",\"mesh\"], \"sleek, futuristic\", [\".com\",\".tech\",\".io\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Nonprofit climate org\", [\"climate\",\"earth\",\"action\"], \"serious, hopeful\", [\".org\",\".earth\",\".com\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Japanese stationery (JP translit)\", [\"pen\",\"paper\",\"kawaii\"], \"cute, refined\", [\".jp\",\".shop\",\".com\"], \"ja\", \"Latin\", \"Transliterated keywords only\"),\n",
        "    (\"Arabic food delivery (translit)\", [\"souk\",\"fresh\",\"sah\"], \"friendly, reliable\", [\".com\",\".me\",\".app\"], \"ar\", \"Latin\", \"Transliterated keywords only\"),\n",
        "    (\"Pet supplements DTC\", [\"pet\",\"chew\",\"boost\"], \"friendly, credible\", [\".com\",\".pet\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Outdoor gear rental\", [\"camp\",\"hike\",\"rent\"], \"adventurous, practical\", [\".com\",\".outdoors\",\".rentals\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Kids coding classes\", [\"code\",\"kids\",\"learn\"], \"playful, educational\", [\".com\",\".school\",\".academy\"], \"en\", \"Latin\", \"\"),\n",
        "]\n",
        "\n",
        "complexity_levels = [\"basic\",\"moderate\",\"advanced\"]\n",
        "\n",
        "def make_briefs(catalog):\n",
        "    briefs = []\n",
        "    for (title, keywords, tone, tlds, lang, script, notes) in catalog:\n",
        "        briefs.append({\n",
        "            \"brief_id\": uid(),\n",
        "            \"title\": title,\n",
        "            \"language\": lang,\n",
        "            \"script\": script,\n",
        "            \"tone\": tone,\n",
        "            \"keywords\": keywords,\n",
        "            \"constraints\": {\n",
        "                \"max_len\": random.choice([10,12,14]),\n",
        "                \"allowed_tlds\": tlds,\n",
        "                \"forbid_digits\": True,\n",
        "                \"forbid_hyphens\": True,\n",
        "                \"ascii_only\": True\n",
        "            },\n",
        "            \"complexity\": random.choice(complexity_levels),\n",
        "            \"notes\": f\"Synthetic brief; availability not verified. {notes}\".strip()\n",
        "        })\n",
        "    return briefs\n",
        "\n",
        "briefs = make_briefs(business_catalog)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Candidate generation\n",
        "# -------------------------\n",
        "\n",
        "# A small syllable bank to form pronounceable nonce words (harmless content only)\n",
        "syllables = [\n",
        "    \"bre\",\"ve\",\"xa\",\"no\",\"va\",\"ly\",\"zo\",\"ri\",\"ta\",\"lo\",\"fi\",\"ki\",\"ra\",\"ne\",\"mi\",\"do\",\"tu\",\"su\",\"pla\",\"tri\",\"quo\",\"zen\",\"lum\",\"sio\",\n",
        "    \"meta\",\"nex\",\"ora\",\"kiri\",\"terra\",\"flux\",\"vanta\",\"pleni\",\"astra\",\"omni\",\"veri\",\"cora\",\"mira\",\"luma\",\"axi\",\"primo\",\"alto\",\"vivo\",\n",
        "    \"nori\",\"lumi\",\"kora\",\"vexa\",\"tava\",\"moro\",\"lino\",\"nexa\",\"pivo\",\"dela\",\"soma\",\"trio\"\n",
        "]\n",
        "\n",
        "def gen_nonce(max_len):\n",
        "    # build pronounceable-ish string; reserve 3 chars for \".tld\"\n",
        "    for _ in range(80):\n",
        "        parts = random.sample(syllables, k=random.choice([2,3]))\n",
        "        name = \"\".join(parts).lower()\n",
        "        name = re.sub(r'(.)\\1{2,}', r'\\1\\1', name)  # compress 3+ repeats\n",
        "        if len(name) <= max_len and name.isascii() and name.isalpha():\n",
        "            return name\n",
        "        # fallback: trim\n",
        "        if len(name) > max_len:\n",
        "            name = name[:max_len]\n",
        "            if name.isalpha():\n",
        "                return name\n",
        "    return \"novexa\"\n",
        "\n",
        "def make_candidates_for_brief(b, k=12):\n",
        "    cands = []\n",
        "    max_len = b[\"constraints\"][\"max_len\"]\n",
        "    allowed_tlds = b[\"constraints\"][\"allowed_tlds\"]\n",
        "\n",
        "    # functions that create intentionally *bad* examples (for training filters)\n",
        "    def inj_digit(base): return base.replace(\"o\",\"0\") + random.choice(allowed_tlds)\n",
        "    def inj_hyphen(base): return base + \"-pro\" + random.choice(allowed_tlds)\n",
        "    def inj_trademark_like(base): return \"go0gle-\" + base + random.choice(allowed_tlds)\n",
        "\n",
        "    bad_funcs = [inj_digit, inj_hyphen, inj_trademark_like]\n",
        "\n",
        "    for i in range(k):\n",
        "        base = gen_nonce(max_len)\n",
        "        tld = random.choice(allowed_tlds)\n",
        "        domain = f\"{base}{tld}\"\n",
        "        rationale = f\"Short coined word aligned to keywords ({', '.join(b['keywords'])}) and tone '{b['tone']}'.\"\n",
        "        safety = {\"flagged\": False, \"reasons\": []}\n",
        "        passes = True\n",
        "\n",
        "        # inject one flawed sample per 6\n",
        "        if (i+1) % 6 == 0:\n",
        "            dom = random.choice(bad_funcs)(base)\n",
        "            domain = dom\n",
        "\n",
        "        # checks\n",
        "        name_part = domain.split(\".\")[0]\n",
        "        if len(name_part) > max_len:\n",
        "            passes = False\n",
        "        if any(ch.isdigit() for ch in domain):\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"contains_digit\")\n",
        "        if \"-\" in domain:\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"contains_hyphen\")\n",
        "        if \"go0gle\" in domain:\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"trademark_like\")\n",
        "\n",
        "        scores = {\n",
        "            \"brandability\": round(random.uniform(0.6, 0.95),2),\n",
        "            \"brevity\": round(max(0.3, 1 - len(name_part)/max(6, max_len)),2),\n",
        "            \"keyword_fit\": round(random.uniform(0.55, 0.9),2)\n",
        "        }\n",
        "        cands.append({\n",
        "            \"candidate_id\": uid(),\n",
        "            \"brief_id\": b[\"brief_id\"],\n",
        "            \"domain\": domain,\n",
        "            \"rationale\": rationale,\n",
        "            \"scores\": scores,\n",
        "            \"passes_constraints\": passes,\n",
        "            \"safety\": safety\n",
        "        })\n",
        "    return cands\n",
        "\n",
        "candidates = []\n",
        "for b in briefs:\n",
        "    candidates.extend(make_candidates_for_brief(b, k=12))\n",
        "\n",
        "# -------------------------\n",
        "# 3) Pairwise preferences (synthetic DPO/IPO data)\n",
        "# -------------------------\n",
        "pairwise = []\n",
        "for b in briefs:\n",
        "    bcands = [c for c in candidates if c[\"brief_id\"] == b[\"brief_id\"]]\n",
        "    ranked = sorted(bcands, key=lambda x: (x[\"passes_constraints\"], x[\"scores\"][\"brandability\"]), reverse=True)\n",
        "    tops = ranked[:4]\n",
        "    bots = ranked[-4:]\n",
        "    for a in tops:\n",
        "        for d in bots:\n",
        "            pairwise.append({\n",
        "                \"pair_id\": uid(),\n",
        "                \"brief_id\": b[\"brief_id\"],\n",
        "                \"winner_candidate_id\": a[\"candidate_id\"],\n",
        "                \"loser_candidate_id\": d[\"candidate_id\"],\n",
        "                \"reason_codes\": [\"brandability\",\"constraint_pass\",\"safety_margin\"]\n",
        "            })\n",
        "\n",
        "# -------------------------\n",
        "# 4) Write files\n",
        "# -------------------------\n",
        "paths = {\n",
        "    \"briefs\": os.path.join(OUT_DIR, \"domain_briefs.jsonl\"),\n",
        "    \"candidates\": os.path.join(OUT_DIR, \"domain_candidates.jsonl\"),\n",
        "    \"pairwise\": os.path.join(OUT_DIR, \"domain_pairwise.jsonl\"),\n",
        "    \"readme\": os.path.join(OUT_DIR, \"README_methodology.md\"),\n",
        "    \"script\": os.path.join(OUT_DIR, \"generate_synthetic_dataset.py\"),\n",
        "}\n",
        "\n",
        "with open(paths[\"briefs\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for b in briefs:\n",
        "        f.write(json.dumps(b, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "with open(paths[\"candidates\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in candidates:\n",
        "        f.write(json.dumps(c, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "with open(paths[\"pairwise\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairwise:\n",
        "        f.write(json.dumps(p, ensure_ascii=False)+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jslu5l08_aAO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development & Iteration\n",
        "• Baseline Model: Fine-tune initial open-source LLM. You can use common recipes for that.\n",
        "\n",
        "• Improved Model(s): Address discovered issues through, i.e.:\n",
        "\n",
        "o Dataset augmentation\n",
        "\n",
        "o Different fine-tuning approaches (LoRA, full fine-tuning, etc.)\n",
        "\n",
        "o Hyperparameter optimization\n",
        "\n",
        "• Save and version all model checkpoints"
      ],
      "metadata": {
        "id": "VWTrsKgeBXp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers>=4.43\" \"datasets>=2.19\" \"accelerate>=0.33\" \"peft>=0.12\" \"bitsandbytes>=0.43\" \"trl>=0.9\" sentencepiece evaluate huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg_NwsZbBmdc",
        "outputId": "bf207bd8-7904-426c-91dc-0db7ddd8ab1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste my HF token\n"
      ],
      "metadata": {
        "id": "-cQ9XQVfBuGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}