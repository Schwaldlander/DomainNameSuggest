{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwwgMM2RnCmMpBs9OwC4sC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schwaldlander/DomainNameSuggest/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a project on domain name suggestion.\n",
        "\n",
        "Proposing a suitable Domain Name is a tricky assignment for entrepreneurs. Clarity, Pronunciation, Popular Reception, Cultural Implications, trademark laws and regulations shall be taken into account.  \n",
        "\n",
        "Targets include:\n",
        "1. Reproducible Performance with Model Version Tracking\n",
        "2. Runnable evaluation framework that works across all model iterations\n",
        "3. **Optional**: Deploy selected model as API endpoint\n",
        "\n",
        "\n",
        "General Setting: GPU required"
      ],
      "metadata": {
        "id": "l8D4uJg2OWPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Step: Create a small, diverse synthetic dataset for domain-name suggestion tasks\n",
        "\n",
        "It includes:\n",
        "1) Briefs (JSONL)\n",
        "\n",
        "2) Candidates with labels (JSONL)\n",
        "\n",
        "3) Pairwise preference judgments (JSONL)\n",
        "\n",
        "4) A README-style methodology (Markdown)\n",
        "\n",
        "\n",
        "# Synthetic Dataset for Domain Name Suggestions\n",
        "\n",
        "**Generated:** {datetime.utcnow().isoformat()}Z\n",
        "\n",
        "## Files\n",
        "- `domain_briefs.jsonl` — Diverse briefs (industry, tone, keywords, constraints, complexity)\n",
        "- `domain_candidates.jsonl` — Candidate suggestions with scores, pass/fail, safety flags\n",
        "- `domain_pairwise.jsonl` — Synthetic pairwise preferences for DPO/IPO\n",
        "\n",
        "## Diversity Coverage\n",
        "- **Business types**: fintech, eco cosmetics, B2B AI, coffee roaster, tutoring (FR), dev tools (DE), travel (ES), wellness, home IoT, climate nonprofit, JP stationery (translit), AR food delivery (translit), pet supplements, outdoor rentals, kids coding.\n",
        "- **Languages/scripts**: EN, FR, DE, ES (Latin). JP/AR represented via **Latin transliteration** to avoid IDN in this first version.\n",
        "- **Complexity levels**: basic, moderate, advanced (randomly assigned) indicating constraints richness and prompt realism.\n",
        "\n",
        "## Methodology\n",
        "1. **Brief Construction**: For each business type, we define language, tone, keywords, and constraints:\n",
        "   - `max_len` (10/12/14), `allowed_tlds` (domain-appropriate),\n",
        "   - forbid digits/hyphens, ASCII-only for v1 (IDN can be added later).\n",
        "2. **Candidate Generation**: Nonce-word generator from a curated syllable bank creates pronounceable, brandable strings. We avoid real brands or adult/illegal terms.\n",
        "3. **Safety & Constraints**: We inject a small fraction of *intentionally flawed* candidates (digits, hyphens, or trademark-like typosquats such as `go0gle-...`) to train and evaluate filters. No explicit harmful content is included.\n",
        "4. **Weak Labels**: Each candidate has pseudo-scores (`brandability`, `brevity`, `keyword_fit`) for reranking studies. Replace with human ratings over time.\n",
        "5. **Pairwise Preferences**: Winners are sampled from higher-scored, constraint-passing candidates; losers from lower-scored/flagged ones. This supports preference optimization (DPO/IPO/KTO).\n",
        "6. **Intended Use**: Bootstrapping a generation→filter→rerank pipeline and automated tests. For production, add multilingual scripts, IDN/homograph checks, and human review.\n",
        "7. **Ethics & Safety**: The dataset purposefully avoids generating or normalizing harmful categories (hate, sexual content, illegal goods/services, self-harm, extremist content). It includes negative examples only in the form of benign constraint violations and obvious trademark-like typosquats to test refusal/filters.\n",
        "\n",
        "## Schemas\n",
        "### Brief\n",
        "```json\n",
        "{{\"brief_id\":\"uuid\",\"title\":\"string\",\"language\":\"en\",\"script\":\"Latin\",\"tone\":\"string\",\"keywords\":[\"k1\",\"k2\"],\"constraints\":{{\"max_len\":12,\"allowed_tlds\":[\".com\",\".io\"],\"forbid_digits\":true,\"forbid_hyphens\":true,\"ascii_only\":true}},\"complexity\":\"basic|moderate|advanced\",\"notes\":\"string\"}}\n",
        "```\n",
        "\n",
        "\n",
        "### Candidate\n",
        "```json\n",
        "{{\"candidate_id\":\"uuid\",\"brief_id\":\"uuid\",\"domain\":\"brevexa.com\",\"rationale\":\"string\",\"scores\":{{\"brandability\":0.85,\"brevity\":0.9,\"keyword_fit\":0.7}},\"passes_constraints\":true,\"safety\":{{\"flagged\":false,\"reasons\":[]}}}}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGlOZ52Ex5ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, uuid, textwrap, os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "HAvnHhEYyaku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m8j_ywSrPTRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json, random, uuid, os, re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def uid():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/domain_suggest/data\"\n",
        "ensure_dir(OUT_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Briefs\n",
        "# -------------------------\n",
        "business_catalog = [\n",
        "    # (title, keywords, tone, tlds, language, script, notes)\n",
        "    (\"Fintech payments wallet\", [\"pay\", \"wallet\", \"secure\"], \"premium, trustworthy\", [\".com\",\".io\",\".pay\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Eco-friendly cosmetics\", [\"vegan\", \"plant\", \"glow\"], \"gentle, natural\", [\".com\",\".beauty\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"B2B AI analytics\", [\"insight\", \"metrics\", \"predict\"], \"modern, technical\", [\".ai\",\".io\",\".com\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Artisanal coffee roaster\", [\"beans\",\"roast\",\"origin\"], \"craft, warm\", [\".com\",\".coffee\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Online French tutoring\", [\"cours\",\"langue\",\"coach\"], \"convivial, sérieux\", [\".fr\",\".com\"], \"fr\", \"Latin\", \"\"),\n",
        "    (\"SaaS developer tools (DE)\", [\"code\",\"build\",\"deploy\"], \"prägnant, professionell\", [\".de\",\".dev\",\".io\"], \"de\", \"Latin\", \"\"),\n",
        "    (\"Travel planning app (ES)\", [\"viaje\",\"ruta\",\"plan\"], \"amable, inspirador\", [\".es\",\".app\",\".com\"], \"es\", \"Latin\", \"\"),\n",
        "    (\"Wellness & yoga studio\", [\"flow\",\"breathe\",\"calm\"], \"soothing, minimalist\", [\".com\",\".studio\",\".fit\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Home automation IoT\", [\"smart\",\"home\",\"mesh\"], \"sleek, futuristic\", [\".com\",\".tech\",\".io\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Nonprofit climate org\", [\"climate\",\"earth\",\"action\"], \"serious, hopeful\", [\".org\",\".earth\",\".com\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Japanese stationery (JP translit)\", [\"pen\",\"paper\",\"kawaii\"], \"cute, refined\", [\".jp\",\".shop\",\".com\"], \"ja\", \"Latin\", \"Transliterated keywords only\"),\n",
        "    (\"Arabic food delivery (translit)\", [\"souk\",\"fresh\",\"sah\"], \"friendly, reliable\", [\".com\",\".me\",\".app\"], \"ar\", \"Latin\", \"Transliterated keywords only\"),\n",
        "    (\"Pet supplements DTC\", [\"pet\",\"chew\",\"boost\"], \"friendly, credible\", [\".com\",\".pet\",\".shop\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Outdoor gear rental\", [\"camp\",\"hike\",\"rent\"], \"adventurous, practical\", [\".com\",\".outdoors\",\".rentals\"], \"en\", \"Latin\", \"\"),\n",
        "    (\"Kids coding classes\", [\"code\",\"kids\",\"learn\"], \"playful, educational\", [\".com\",\".school\",\".academy\"], \"en\", \"Latin\", \"\"),\n",
        "]\n",
        "\n",
        "complexity_levels = [\"basic\",\"moderate\",\"advanced\"]\n",
        "\n",
        "def make_briefs(catalog):\n",
        "    briefs = []\n",
        "    for (title, keywords, tone, tlds, lang, script, notes) in catalog:\n",
        "        briefs.append({\n",
        "            \"brief_id\": uid(),\n",
        "            \"title\": title,\n",
        "            \"language\": lang,\n",
        "            \"script\": script,\n",
        "            \"tone\": tone,\n",
        "            \"keywords\": keywords,\n",
        "            \"constraints\": {\n",
        "                \"max_len\": random.choice([10,12,14]),\n",
        "                \"allowed_tlds\": tlds,\n",
        "                \"forbid_digits\": True,\n",
        "                \"forbid_hyphens\": True,\n",
        "                \"ascii_only\": True\n",
        "            },\n",
        "            \"complexity\": random.choice(complexity_levels),\n",
        "            \"notes\": f\"Synthetic brief; availability not verified. {notes}\".strip()\n",
        "        })\n",
        "    return briefs\n",
        "\n",
        "briefs = make_briefs(business_catalog)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Candidate generation\n",
        "# -------------------------\n",
        "\n",
        "# A small syllable bank to form pronounceable nonce words (harmless content only)\n",
        "syllables = [\n",
        "    \"bre\",\"ve\",\"xa\",\"no\",\"va\",\"ly\",\"zo\",\"ri\",\"ta\",\"lo\",\"fi\",\"ki\",\"ra\",\"ne\",\"mi\",\"do\",\"tu\",\"su\",\"pla\",\"tri\",\"quo\",\"zen\",\"lum\",\"sio\",\n",
        "    \"meta\",\"nex\",\"ora\",\"kiri\",\"terra\",\"flux\",\"vanta\",\"pleni\",\"astra\",\"omni\",\"veri\",\"cora\",\"mira\",\"luma\",\"axi\",\"primo\",\"alto\",\"vivo\",\n",
        "    \"nori\",\"lumi\",\"kora\",\"vexa\",\"tava\",\"moro\",\"lino\",\"nexa\",\"pivo\",\"dela\",\"soma\",\"trio\"\n",
        "]\n",
        "\n",
        "def gen_nonce(max_len):\n",
        "    # build pronounceable-ish string; reserve 3 chars for \".tld\"\n",
        "    for _ in range(80):\n",
        "        parts = random.sample(syllables, k=random.choice([2,3]))\n",
        "        name = \"\".join(parts).lower()\n",
        "        name = re.sub(r'(.)\\1{2,}', r'\\1\\1', name)  # compress 3+ repeats\n",
        "        if len(name) <= max_len and name.isascii() and name.isalpha():\n",
        "            return name\n",
        "        # fallback: trim\n",
        "        if len(name) > max_len:\n",
        "            name = name[:max_len]\n",
        "            if name.isalpha():\n",
        "                return name\n",
        "    return \"novexa\"\n",
        "\n",
        "def make_candidates_for_brief(b, k=12):\n",
        "    cands = []\n",
        "    max_len = b[\"constraints\"][\"max_len\"]\n",
        "    allowed_tlds = b[\"constraints\"][\"allowed_tlds\"]\n",
        "\n",
        "    # functions that create intentionally *bad* examples (for training filters)\n",
        "    def inj_digit(base): return base.replace(\"o\",\"0\") + random.choice(allowed_tlds)\n",
        "    def inj_hyphen(base): return base + \"-pro\" + random.choice(allowed_tlds)\n",
        "    def inj_trademark_like(base): return \"go0gle-\" + base + random.choice(allowed_tlds)\n",
        "\n",
        "    bad_funcs = [inj_digit, inj_hyphen, inj_trademark_like]\n",
        "\n",
        "    for i in range(k):\n",
        "        base = gen_nonce(max_len)\n",
        "        tld = random.choice(allowed_tlds)\n",
        "        domain = f\"{base}{tld}\"\n",
        "        rationale = f\"Short coined word aligned to keywords ({', '.join(b['keywords'])}) and tone '{b['tone']}'.\"\n",
        "        safety = {\"flagged\": False, \"reasons\": []}\n",
        "        passes = True\n",
        "\n",
        "        # inject one flawed sample per 6\n",
        "        if (i+1) % 6 == 0:\n",
        "            dom = random.choice(bad_funcs)(base)\n",
        "            domain = dom\n",
        "\n",
        "        # checks\n",
        "        name_part = domain.split(\".\")[0]\n",
        "        if len(name_part) > max_len:\n",
        "            passes = False\n",
        "        if any(ch.isdigit() for ch in domain):\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"contains_digit\")\n",
        "        if \"-\" in domain:\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"contains_hyphen\")\n",
        "        if domain in [\"go0gle\" ,\"yah0o\", \"nazi\"]:\n",
        "            passes = False; safety[\"flagged\"]=True; safety[\"reasons\"].append(\"trademark_like\")\n",
        "\n",
        "        scores = {\n",
        "            \"brandability\": round(random.uniform(0.6, 0.95),2),\n",
        "            \"brevity\": round(max(0.3, 1 - len(name_part)/max(6, max_len)),2),\n",
        "            \"keyword_fit\": round(random.uniform(0.55, 0.9),2)\n",
        "        }\n",
        "        cands.append({\n",
        "            \"candidate_id\": uid(),\n",
        "            \"brief_id\": b[\"brief_id\"],\n",
        "            \"domain\": domain,\n",
        "            \"rationale\": rationale,\n",
        "            \"scores\": scores,\n",
        "            \"passes_constraints\": passes,\n",
        "            \"safety\": safety\n",
        "        })\n",
        "    return cands\n",
        "\n",
        "candidates = []\n",
        "for b in briefs:\n",
        "    candidates.extend(make_candidates_for_brief(b, k=12))\n",
        "\n",
        "# -------------------------\n",
        "# 3) Pairwise preferences (synthetic DPO/IPO data)\n",
        "# -------------------------\n",
        "pairwise = []\n",
        "for b in briefs:\n",
        "    bcands = [c for c in candidates if c[\"brief_id\"] == b[\"brief_id\"]]\n",
        "    ranked = sorted(bcands, key=lambda x: (x[\"passes_constraints\"], x[\"scores\"][\"brandability\"]), reverse=True)\n",
        "    tops = ranked[:4]\n",
        "    bots = ranked[-4:]\n",
        "    for a in tops:\n",
        "        for d in bots:\n",
        "            pairwise.append({\n",
        "                \"pair_id\": uid(),\n",
        "                \"brief_id\": b[\"brief_id\"],\n",
        "                \"winner_candidate_id\": a[\"candidate_id\"],\n",
        "                \"loser_candidate_id\": d[\"candidate_id\"],\n",
        "                \"reason_codes\": [\"brandability\",\"constraint_pass\",\"safety_margin\"]\n",
        "            })\n",
        "\n",
        "# -------------------------\n",
        "# 4) Write files\n",
        "# -------------------------\n",
        "paths = {\n",
        "    \"briefs\": os.path.join(OUT_DIR, \"domain_briefs.jsonl\"),\n",
        "    \"candidates\": os.path.join(OUT_DIR, \"domain_candidates.jsonl\"),\n",
        "    \"pairwise\": os.path.join(OUT_DIR, \"domain_pairwise.jsonl\"),\n",
        "    \"readme\": os.path.join(OUT_DIR, \"README_methodology.md\"),\n",
        "    \"script\": os.path.join(OUT_DIR, \"generate_synthetic_dataset.py\"),\n",
        "}\n",
        "\n",
        "# if path not exist\n",
        "if not os.path.exists(paths[\"briefs\"]):\n",
        "  with open(paths[\"briefs\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for b in briefs:\n",
        "        f.write(json.dumps(b, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "if not os.path.exists(paths[\"candidates\"]):\n",
        "  with open(paths[\"candidates\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in candidates:\n",
        "        f.write(json.dumps(c, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "if not os.path.exists(path[])\n",
        "with open(paths[\"pairwise\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairwise:\n",
        "        f.write(json.dumps(p, ensure_ascii=False)+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jslu5l08_aAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development & Iteration\n",
        "• Baseline Model: Fine-tune initial open-source LLM. You can use common recipes for that.\n",
        "\n",
        "• Improved Model(s): Address discovered issues through, i.e.:\n",
        "\n",
        "o Dataset augmentation\n",
        "\n",
        "o Different fine-tuning approaches (LoRA, full fine-tuning, etc.)\n",
        "\n",
        "o Hyperparameter optimization\n",
        "\n",
        "• Save and version all model checkpoints"
      ],
      "metadata": {
        "id": "VWTrsKgeBXp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers>=4.43\" \"datasets>=2.19\" \"accelerate>=0.33\" \"peft>=0.12\" \"bitsandbytes>=0.43\" \"trl>=0.9\" sentencepiece evaluate huggingface_hub\n"
      ],
      "metadata": {
        "id": "vg_NwsZbBmdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste my HF token\n"
      ],
      "metadata": {
        "id": "-cQ9XQVfBuGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, os, pandas as pd, datasets as ds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/domain_suggest/data\"  # put your files here\n",
        "briefs_path = f\"{DATA_DIR}/domain_briefs.jsonl\"\n",
        "cands_path  = f\"{DATA_DIR}/domain_candidates.jsonl\"\n",
        "pairs_path  = f\"{DATA_DIR}/domain_pairwise.jsonl\"\n",
        "\n",
        "# Build SFT samples: (brief) -> (JSON suggestions)\n",
        "# We'll group candidates by brief and produce small top-k sets as targets.\n",
        "def load_briefs(path):\n",
        "    return [json.loads(x) for x in open(path, \"r\", encoding=\"utf-8\").read().splitlines()]\n",
        "\n",
        "def load_cands(path):\n",
        "    return [json.loads(x) for x in open(path, \"r\", encoding=\"utf-8\").read().splitlines()]\n",
        "\n",
        "briefs = load_briefs(briefs_path)\n",
        "cands  = load_cands(cands_path)\n",
        "\n",
        "# Build one training example per brief: prompt = brief (YAML-ish), target = JSON with k suggestions\n",
        "by_brief = {}\n",
        "for c in cands:\n",
        "    by_brief.setdefault(c[\"brief_id\"], []).append(c)\n",
        "\n",
        "samples = []\n",
        "K = 6  # number of suggestions to train on\n",
        "for b in briefs:\n",
        "    group = by_brief.get(b[\"brief_id\"], [])\n",
        "    # choose valid, constraint-passing first; backfill with others if needed\n",
        "    good = [x for x in group if x.get(\"passes_constraints\", False) and not x[\"safety\"][\"flagged\"]]\n",
        "    pool = good if len(good) >= K else (good + [x for x in group if x not in good])\n",
        "    sel = sorted(pool, key=lambda x: x[\"scores\"][\"brandability\"], reverse=True)[:K]\n",
        "\n",
        "    prompt = f\"\"\"You are a brand-safe domain name generator.\n",
        "Follow the policy: refuse unsafe requests; output valid JSON schema only.\n",
        "\n",
        "[BRIEF]\n",
        "title: {b['title']}\n",
        "language: {b['language']}\n",
        "tone: {b['tone']}\n",
        "keywords: {', '.join(b['keywords'])}\n",
        "constraints:\n",
        "  max_len: {b['constraints']['max_len']}\n",
        "  allowed_tlds: {', '.join(b['constraints']['allowed_tlds'])}\n",
        "  forbid_digits: {b['constraints']['forbid_digits']}\n",
        "  forbid_hyphens: {b['constraints']['forbid_hyphens']}\n",
        "  ascii_only: {b['constraints']['ascii_only']}\n",
        "\"\"\"\n",
        "\n",
        "    target = {\n",
        "        \"query_id\": b[\"brief_id\"],\n",
        "        \"suggestions\": [\n",
        "            {\n",
        "                \"domain\": s[\"domain\"].split(\".\")[0] + s[\"domain\"][len(s[\"domain\"].split(\".\")[0]):],\n",
        "                \"rationale\": s[\"rationale\"],\n",
        "                \"scores\": s[\"scores\"],\n",
        "                \"safety\": s[\"safety\"],\n",
        "            } for s in sel\n",
        "        ],\n",
        "        \"notes\": [\"Availability not verified\"]\n",
        "    }\n",
        "    samples.append({\"prompt\": prompt.strip(), \"response\": json.dumps(target, ensure_ascii=False)})\n",
        "\n",
        "train, test = train_test_split(samples, test_size=0.15, random_state=7)\n",
        "val, test  = train_test_split(test, test_size=0.5, random_state=7)\n",
        "\n",
        "dataset = ds.DatasetDict({\n",
        "    \"train\": ds.Dataset.from_list(train),\n",
        "    \"validation\": ds.Dataset.from_list(val),\n",
        "    \"test\": ds.Dataset.from_list(test)\n",
        "})\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "HNfg-bShNG3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer\n",
        "import torch, os, json\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/domain_suggest/checkpoints/baseline_qlora\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True) #padding_side=\"left\"\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "# IMPORTANT: prepare for k-bit training + gradient checkpointing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wH9vYSTYNM50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "peft_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # common names; adjusts per model\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# Format to dialogue-style prompt → response!\n",
        "def format_example(ex):\n",
        "    sys = \"You generate brand-safe domain suggestions and strictly refuse unsafe requests. Output strict JSON.\"\n",
        "    user = ex[\"prompt\"]\n",
        "    assistant = ex[\"response\"]\n",
        "    # simple chat template\n",
        "    text = f\"<|im_start|>system\\n{sys}\\n<|im_end|>\\n<|im_start|>user\\n{user}\\n<|im_end|>\\n<|im_start|>assistant\\n{assistant}\\n<|im_end|>\"\n",
        "    return {\"input_ids\": tok(text, truncation=True, max_length=2048, padding=\"max_length\")[\"input_ids\"]}\n",
        "\n",
        "tokenized = dataset.map(format_example, remove_columns=dataset[\"train\"].column_names, num_proc=1)\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n"
      ],
      "metadata": {
        "id": "lZxqKttvUOFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    gradient_checkpointing=True,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    data_collator=collator\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save PEFT checkpoint (LoRA weights) + tokenizer\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tok.save_pretrained(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "lair_KcCRHAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(tok, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "def gen(brief):\n",
        "    prompt = f\"\"\"title: {brief['title']}\n",
        "language: {brief['language']}\n",
        "tone: {brief['tone']}\n",
        "keywords: {', '.join(brief['keywords'])}\n",
        "constraints:\n",
        "  max_len: {brief['constraints']['max_len']}\n",
        "  allowed_tlds: {', '.join(brief['constraints']['allowed_tlds'])}\n",
        "  forbid_digits: {brief['constraints']['forbid_digits']}\n",
        "  forbid_hyphens: {brief['constraints']['forbid_hyphens']}\n",
        "  ascii_only: {brief['constraints']['ascii_only']}\"\"\"\n",
        "\n",
        "    sys = \"You generate brand-safe domain suggestions and strictly refuse unsafe requests. Output strict JSON only.\"\n",
        "    text = f\"<|im_start|>system\\n{sys}\\n<|im_end|>\\n<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    inputs = tok(text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=400, do_sample=True, temperature=0.7, top_p=0.9, streamer=streamer)\n",
        "    print()  # newline\n",
        "\n",
        "gen(random.choice(briefs))\n"
      ],
      "metadata": {
        "id": "WjA0r_t9pVlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import math, numpy as np\n",
        "\n",
        "sweep = [\n",
        "  {\"r\": 8,  \"lr\": 2e-4, \"epochs\": 2},\n",
        "  {\"r\": 16, \"lr\": 1e-4, \"epochs\": 2},\n",
        "  {\"r\": 32, \"lr\": 8e-5, \"epochs\": 3},\n",
        "]\n",
        "results = []\n",
        "\n",
        "for i, hp in enumerate(sweep, 1):\n",
        "    out = f\"/content/drive/MyDrive/domain_suggest/checkpoints/qlora_sweep_run{i}\"\n",
        "    cfg = deepcopy(peft_cfg)\n",
        "    cfg.r = hp[\"r\"]\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_cfg, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "    model = get_peft_model(model, cfg)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=hp[\"lr\"],\n",
        "        num_train_epochs=hp[\"epochs\"],\n",
        "        logging_steps=25,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        bf16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(model=model, args=args, train_dataset=tokenized[\"train\"], eval_dataset=tokenized[\"validation\"], data_collator=collator)\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    results.append({\"run\": i, **hp, **metrics})\n",
        "\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "fJD6bDrypc6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [json.loads(x) for x in open(pairs_path, \"r\", encoding=\"utf-8\").read().splitlines()]\n",
        "cand_map = {c[\"candidate_id\"]: c for c in cands}\n",
        "brief_map = {b[\"brief_id\"]: b for b in briefs}\n",
        "\n",
        "def dpo_rows(pairs):\n",
        "    rows = []\n",
        "    for p in pairs:\n",
        "        b = brief_map[p[\"brief_id\"]]\n",
        "        w = cand_map[p[\"winner_candidate_id\"]]\n",
        "        l = cand_map[p[\"loser_candidate_id\"]]\n",
        "        prompt = f\"\"\"Provide JSON suggestions for this brief.\n",
        "\n",
        "title: {b['title']}\n",
        "language: {b['language']}\n",
        "tone: {b['tone']}\n",
        "keywords: {', '.join(b['keywords'])}\n",
        "constraints:\n",
        "  max_len: {b['constraints']['max_len']}\n",
        "  allowed_tlds: {', '.join(b['constraints']['allowed_tlds'])}\n",
        "  forbid_digits: {b['constraints']['forbid_digits']}\n",
        "  forbid_hyphens: {b['constraints']['forbid_hyphens']}\n",
        "  ascii_only: {b['constraints']['ascii_only']}\"\"\"\n",
        "\n",
        "        # Format chosen/rejected as JSON single-item suggestions to keep sequences short\n",
        "        def to_json(c):\n",
        "            return json.dumps({\n",
        "                \"query_id\": b[\"brief_id\"],\n",
        "                \"suggestions\": [{\n",
        "                    \"domain\": c[\"domain\"],\n",
        "                    \"rationale\": c[\"rationale\"],\n",
        "                    \"scores\": c[\"scores\"],\n",
        "                    \"safety\": c[\"safety\"]\n",
        "                }]\n",
        "            }, ensure_ascii=False)\n",
        "\n",
        "        rows.append({\"prompt\": prompt, \"chosen\": to_json(w), \"rejected\": to_json(l)})\n",
        "    return rows\n",
        "\n",
        "dpo_data = ds.Dataset.from_list(dpo_rows(pairs))\n",
        "dpo_data = dpo_data.train_test_split(test_size=0.1, seed=7)\n",
        "dpo_data\n"
      ],
      "metadata": {
        "id": "KtyI2rnQpf8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# Load the baseline SFT adapter as the starting point (policy init)\n",
        "policy = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_cfg, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "policy = get_peft_model(policy, peft_cfg)  # same config as baseline\n",
        "policy.load_adapter(OUTPUT_DIR)            # load LoRA weights from baseline\n",
        "\n",
        "dpo_args = DPOConfig(\n",
        "    output_dir=\"/content/drive/MyDrive/domain_suggest/checkpoints/dpo_v1\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5e-6,\n",
        "    num_train_epochs=1,\n",
        "    beta=0.1,  # DPO temperature\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=policy,\n",
        "    args=dpo_args,\n",
        "    beta=dpo_args.beta,\n",
        "    train_dataset=dpo_data[\"train\"],\n",
        "    eval_dataset=None,\n",
        "    tokenizer=tok,\n",
        "    max_length=1792,\n",
        "    max_target_length=384,\n",
        "    max_prompt_length=1024\n",
        ")\n",
        "dpo_trainer.train()\n",
        "dpo_trainer.save_model(\"/content/drive/MyDrive/domain_suggest/checkpoints/dpo_v1\")\n",
        "tok.save_pretrained(\"/content/drive/MyDrive/domain_suggest/checkpoints/dpo_v1\")\n"
      ],
      "metadata": {
        "id": "oB2EmqhFpzBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os\n",
        "def record_version(path, tag, parent, notes, metrics):\n",
        "    info = {\n",
        "        \"tag\": tag,\n",
        "        \"timestamp\": time.time(),\n",
        "        \"parent\": parent,\n",
        "        \"notes\": notes,\n",
        "        \"metrics\": metrics\n",
        "    }\n",
        "    with open(os.path.join(path, \"model_version.json\"), \"w\") as f:\n",
        "        json.dump(info, f, indent=2)\n",
        "\n",
        "# Example:\n",
        "record_version(\"/content/drive/MyDrive/domain_suggest/checkpoints/baseline_qlora\", \"v0.1-baseline-qlora\", None, \"Initial SFT QLoRA\", {\"val_loss\": trainer.state.log_history[-1].get(\"eval_loss\", None)})\n"
      ],
      "metadata": {
        "id": "rsCE5Rvup194"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo, upload_folder\n",
        "\n",
        "org_or_user = \"your-hf-username\"\n",
        "repo_name = \"domain-suggester-qwen25-3b\"\n",
        "repo_id = f\"{org_or_user}/{repo_name}\"\n",
        "\n",
        "# Create once:\n",
        "# create_repo(repo_id, private=True)\n",
        "\n",
        "# Upload each run:\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/domain_suggest/checkpoints/baseline_qlora\",\n",
        "    repo_id=repo_id,\n",
        "    path_in_repo=\"v0.1-baseline-qlora\",\n",
        ")\n",
        "# Add a tag by creating a release or using \"refs/tags/<tag>\" via git if you clone the repo.\n"
      ],
      "metadata": {
        "id": "7V8mmBlhp4ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM-as-a-Judge Evaluation Framework (Colab-ready files)\n",
        "# - Creates a reusable module with rubric, rule checks, and multiple judge backends:\n",
        "#   * RuleBasedJudge (works offline; useful as a fallback & sanity check)\n",
        "#   * OpenAIJudge / AnthropicJudge stubs (for API-based LLM evaluation)\n",
        "#   * LocalHFJudge (for open-source models if you want to run locally with Transformers)\n",
        "# - Includes a runner script that evaluates a dataset (from Step 1) and writes:\n",
        "#   * evaluation_report.jsonl (per-suggestion breakdown)\n",
        "#   * evaluation_summary.md (aggregated metrics)\n",
        "#\n",
        "# In this environment we cannot call external APIs or download models,\n",
        "# so the demo below will run the RuleBasedJudge on your dataset.\n",
        "#\n",
        "# You can download and use these files directly in Colab.\n",
        "\n",
        "import os, json, math, statistics, re, uuid, random, textwrap\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/mnt/data/llm_judge_framework\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------\n",
        "# 1) llm_judge.py (framework)\n",
        "# ------------------------------\n",
        "llm_judge_py = r'''\n",
        "\"\"\"\n",
        "llm_judge.py — LLM-as-a-Judge framework for domain name suggestions\n",
        "\n",
        "Backends:\n",
        "- RuleBasedJudge: offline heuristic scorer (baseline + sanity check)\n",
        "- OpenAIJudge: calls OpenAI Chat Completions API (requires OPENAI_API_KEY)\n",
        "- AnthropicJudge: calls Anthropic Messages API (requires ANTHROPIC_API_KEY)\n",
        "- LocalHFJudge: runs an open-source judge with transformers (optional)\n",
        "\n",
        "Outputs:\n",
        "- Per-suggestion JSON records with rubric scores and a weighted final_score.\n",
        "- Aggregated summary metrics helper.\n",
        "\n",
        "NOTE: Availability/trademark checks are out of scope for this judge.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, re, math, random, statistics, time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# --------------------\n",
        "# Rubric & weighting\n",
        "# --------------------\n",
        "\n",
        "RUBRIC_WEIGHTS = {\n",
        "    # Gating: if \"safety\" or \"spec_compliance\" fails (0), final_score = 0\n",
        "    \"brandability\": 0.20,\n",
        "    \"relevance\": 0.20,         # keyword/industry fit\n",
        "    \"pronounceability\": 0.15,\n",
        "    \"memorability\": 0.10,\n",
        "    \"novelty\": 0.10,\n",
        "    \"clarity\": 0.10,           # avoids ambiguity/confusables\n",
        "    \"tld_fit\": 0.05,\n",
        "    \"structure_quality\": 0.10  # JSON formatting & rationale helpfulness\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class Brief:\n",
        "    brief_id: str\n",
        "    title: str\n",
        "    language: str\n",
        "    tone: str\n",
        "    keywords: List[str]\n",
        "    constraints: Dict[str, Any]\n",
        "    notes: str = \"\"\n",
        "\n",
        "@dataclass\n",
        "class Candidate:\n",
        "    candidate_id: str\n",
        "    brief_id: str\n",
        "    domain: str\n",
        "    rationale: str\n",
        "    passes_constraints: bool\n",
        "    safety: Dict[str, Any]\n",
        "\n",
        "# --------------------\n",
        "# Helper: spec checks\n",
        "# --------------------\n",
        "\n",
        "VOWELS = set(\"aeiou\")\n",
        "AMBIGUOUS = set(list(\"l1I0O5S\"))\n",
        "\n",
        "def spec_checks(brief: Brief, domain: str) -> Tuple[bool, List[str]]:\n",
        "    reasons = []\n",
        "    name, *rest = domain.split(\".\")\n",
        "    max_len = brief.constraints.get(\"max_len\", 12)\n",
        "    allowed = brief.constraints.get(\"allowed_tlds\", [])\n",
        "    forbid_digits = brief.constraints.get(\"forbid_digits\", True)\n",
        "    forbid_hyphens = brief.constraints.get(\"forbid_hyphens\", True)\n",
        "    ascii_only = brief.constraints.get(\"ascii_only\", True)\n",
        "\n",
        "    if len(name) > max_len:\n",
        "        reasons.append(\"length_exceeded\")\n",
        "    if forbid_digits and any(ch.isdigit() for ch in name):\n",
        "        reasons.append(\"digits_forbidden\")\n",
        "    if forbid_hyphens and \"-\" in name:\n",
        "        reasons.append(\"hyphen_forbidden\")\n",
        "    if ascii_only and not name.isascii():\n",
        "        reasons.append(\"non_ascii\")\n",
        "    if allowed:\n",
        "        tld = \".\" + domain.split(\".\")[-1]\n",
        "        if tld not in allowed:\n",
        "            reasons.append(\"tld_not_allowed\")\n",
        "    ok = len(reasons) == 0\n",
        "    return ok, reasons\n",
        "\n",
        "# --------------------\n",
        "# Heuristic proxies\n",
        "# --------------------\n",
        "\n",
        "def pronounceability_score(name: str) -> float:\n",
        "    # Simple heuristic: penalize long consonant runs; reward vowel presence\n",
        "    if not name: return 0.0\n",
        "    runs = re.findall(r\"[^aeiou]+\", name)\n",
        "    max_run = max((len(r) for r in runs), default=0)\n",
        "    vowel_ratio = sum(1 for c in name if c in VOWELS) / max(1, len(name))\n",
        "    score = 0.6 * (1 - min(max_run/5, 1)) + 0.4 * min(vowel_ratio/0.4, 1)\n",
        "    return max(0.0, min(1.0, score))\n",
        "\n",
        "def memorability_score(name: str) -> float:\n",
        "    # Shorter, with some repetition but not too much\n",
        "    if not name: return 0.0\n",
        "    length = len(name)\n",
        "    unique_ratio = len(set(name))/length\n",
        "    repeat_penalty = 1 - abs(unique_ratio - 0.7)  # prefer ~0.7 unique ratio\n",
        "    base = max(0.0, 1 - (length - 6)/10)  # 6..16\n",
        "    score = 0.6*base + 0.4*repeat_penalty\n",
        "    return max(0.0, min(1.0, score))\n",
        "\n",
        "def clarity_score(name: str) -> float:\n",
        "    # Penalize ambiguous chars (l/1/I, 0/O, 5/S)\n",
        "    amb_count = sum(1 for c in name if c in AMBIGUOUS)\n",
        "    score = max(0.0, 1 - amb_count / max(4, len(name)/2))\n",
        "    return max(0.0, min(1.0, score))\n",
        "\n",
        "def novelty_score(name: str, keywords: List[str]) -> float:\n",
        "    # Prefer candidates that are not raw keywords; mild penalty for substring overlap\n",
        "    n = name.lower()\n",
        "    overlaps = sum(1 for k in keywords if k.lower() in n)\n",
        "    return max(0.0, min(1.0, 1 - overlaps*0.3))\n",
        "\n",
        "def relevance_score(name: str, keywords: List[str], rationale: str) -> float:\n",
        "    # Proxy: keyword substrings in rationale OR name boosts score\n",
        "    text = (name + \" \" + rationale).lower()\n",
        "    hits = sum(1 for k in keywords if k.lower() in text)\n",
        "    return max(0.0, min(1.0, hits / max(1, len(keywords))))\n",
        "\n",
        "def brandability_score(name: str) -> float:\n",
        "    # Blend of pronounceability + memorability + absence of digits/hyphen\n",
        "    base = 0.5*pronounceability_score(name) + 0.5*memorability_score(name)\n",
        "    if any(ch.isdigit() for ch in name) or \"-\" in name:\n",
        "        base *= 0.7\n",
        "    return max(0.0, min(1.0, base))\n",
        "\n",
        "def tld_fit_score(domain: str, allowed_tlds: List[str]) -> float:\n",
        "    if not allowed_tlds: return 1.0\n",
        "    tld = \".\" + domain.split(\".\")[-1]\n",
        "    return 1.0 if tld in allowed_tlds else 0.0\n",
        "\n",
        "def structure_quality_score(rationale: str) -> float:\n",
        "    # Very rough: presence of short justification, no obvious profanity (not exhaustive)\n",
        "    txt = rationale.strip().lower()\n",
        "    if not txt: return 0.2\n",
        "    return max(0.2, min(1.0, 0.5 + min(len(txt), 200)/400))\n",
        "\n",
        "# --------------------\n",
        "# Judges\n",
        "# --------------------\n",
        "\n",
        "class BaseJudge:\n",
        "    def score_candidate(self, brief: Brief, cand: Candidate) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate_scores(scores: Dict[str, float], gates: Dict[str, bool]) -> float:\n",
        "        if not gates.get(\"safety_ok\", True): return 0.0\n",
        "        if not gates.get(\"spec_ok\", True): return 0.0\n",
        "        total = 0.0\n",
        "        for k, w in RUBRIC_WEIGHTS.items():\n",
        "            total += w * max(0.0, min(1.0, scores.get(k, 0.0)))\n",
        "        return round(total, 4)\n",
        "\n",
        "class RuleBasedJudge(BaseJudge):\n",
        "    def score_candidate(self, brief: Brief, cand: Candidate) -> Dict[str, Any]:\n",
        "        name = cand.domain.split(\".\")[0].lower()\n",
        "        spec_ok, spec_reasons = spec_checks(brief, cand.domain)\n",
        "        safety_ok = not cand.safety.get(\"flagged\", False)\n",
        "\n",
        "        scores = {\n",
        "            \"brandability\": brandability_score(name),\n",
        "            \"relevance\": relevance_score(name, brief.keywords, cand.rationale),\n",
        "            \"pronounceability\": pronounceability_score(name),\n",
        "            \"memorability\": memorability_score(name),\n",
        "            \"novelty\": novelty_score(name, brief.keywords),\n",
        "            \"clarity\": clarity_score(name),\n",
        "            \"tld_fit\": tld_fit_score(cand.domain, brief.constraints.get(\"allowed_tlds\", [])),\n",
        "            \"structure_quality\": structure_quality_score(cand.rationale),\n",
        "        }\n",
        "        gates = {\"safety_ok\": safety_ok, \"spec_ok\": spec_ok}\n",
        "        final = self.aggregate_scores(scores, gates)\n",
        "        return {\n",
        "            \"scores\": scores,\n",
        "            \"gates\": gates,\n",
        "            \"final_score\": final,\n",
        "            \"spec_reasons\": spec_reasons,\n",
        "            \"safety_reasons\": cand.safety.get(\"reasons\", []),\n",
        "            \"judge_backend\": \"rule_based\"\n",
        "        }\n",
        "\n",
        "# --- Placeholders for real LLM judges ---\n",
        "\n",
        "class OpenAIJudge(BaseJudge):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "        judge = OpenAIJudge(model=\"gpt-4o-mini\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "        result = judge.score_candidate(brief, cand)\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str, api_key: Optional[str] = None):\n",
        "        self.model = model\n",
        "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    def _build_prompt(self, brief: Brief, cand: Candidate) -> list:\n",
        "        system = (\n",
        "            \"You are an expert brand evaluator. Score a proposed domain name using the rubric.\"\n",
        "            \" Refuse to endorse unsafe/inappropriate content. Do NOT reveal chain-of-thought.\"\n",
        "            \" Return strict JSON with numeric scores 0..1 and one-sentence reasons.\"\n",
        "        )\n",
        "        user = f\"\"\"\n",
        "[BRIEF]\n",
        "title: {brief.title}\n",
        "language: {brief.language}\n",
        "tone: {brief.tone}\n",
        "keywords: {\", \".join(brief.keywords)}\n",
        "constraints:\n",
        "  max_len: {brief.constraints.get(\"max_len\")}\n",
        "  allowed_tlds: {\", \".join(brief.constraints.get(\"allowed_tlds\", []))}\n",
        "  forbid_digits: {brief.constraints.get(\"forbid_digits\")}\n",
        "  forbid_hyphens: {brief.constraints.get(\"forbid_hyphens\")}\n",
        "  ascii_only: {brief.constraints.get(\"ascii_only\")}\n",
        "\n",
        "[CANDIDATE]\n",
        "domain: {cand.domain}\n",
        "rationale: {cand.rationale}\n",
        "\n",
        "[OUTPUT_JSON_SCHEMA]\n",
        "{{\n",
        "  \"gates\": {{\"safety_ok\": true, \"spec_ok\": true}},\n",
        "  \"scores\": {{\n",
        "    \"brandability\": 0.0, \"relevance\": 0.0, \"pronounceability\": 0.0, \"memorability\": 0.0,\n",
        "    \"novelty\": 0.0, \"clarity\": 0.0, \"tld_fit\": 0.0, \"structure_quality\": 0.0\n",
        "  }},\n",
        "  \"reasons\": {{\n",
        "    \"brandability\": \"\", \"relevance\": \"\", \"pronounceability\": \"\", \"memorability\": \"\",\n",
        "    \"novelty\": \"\", \"clarity\": \"\", \"tld_fit\": \"\", \"structure_quality\": \"\", \"safety\": \"\", \"spec\": \"\"\n",
        "  }}\n",
        "}}\n",
        "Only output JSON, nothing else.\n",
        "\"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user.strip()}\n",
        "        ]\n",
        "\n",
        "    def score_candidate(self, brief: Brief, cand: Candidate) -> Dict[str, Any]:\n",
        "        import requests, json as _json\n",
        "        url = \"https://api.openai.com/v1/chat/completions\"\n",
        "        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": self._build_prompt(brief, cand),\n",
        "            \"temperature\": 0.2\n",
        "        }\n",
        "        resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        content = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        try:\n",
        "            out = _json.loads(content)\n",
        "        except Exception:\n",
        "            out = {\"gates\": {\"safety_ok\": True, \"spec_ok\": True}, \"scores\": {}, \"reasons\": {\"parse_error\": content}}\n",
        "        # Aggregate\n",
        "        final = BaseJudge.aggregate_scores(out.get(\"scores\", {}), out.get(\"gates\", {}))\n",
        "        out[\"final_score\"] = final\n",
        "        out[\"judge_backend\"] = \"openai\"\n",
        "        return out\n",
        "\n",
        "class AnthropicJudge(BaseJudge):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "        judge = AnthropicJudge(model=\"claude-3-5-sonnet\", api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str, api_key: Optional[str] = None):\n",
        "        self.model = model\n",
        "        self.api_key = api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "    def score_candidate(self, brief: Brief, cand: Candidate) -> Dict[str, Any]:\n",
        "        import requests, json as _json\n",
        "        url = \"https://api.anthropic.com/v1/messages\"\n",
        "        headers = {\n",
        "            \"x-api-key\": self.api_key,\n",
        "            \"anthropic-version\": \"2023-06-01\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "        system = \"You are an expert brand evaluator... (same policy as OpenAIJudge).\"\n",
        "        user = \"Same formatted prompt as in OpenAIJudge._build_prompt\"\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"system\": system,\n",
        "            \"messages\": [{\"role\":\"user\",\"content\": user}],\n",
        "            \"max_tokens\": 400,\n",
        "            \"temperature\": 0.2\n",
        "        }\n",
        "        resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        text = resp.json()[\"content\"][0][\"text\"]\n",
        "        try:\n",
        "            out = _json.loads(text)\n",
        "        except Exception:\n",
        "            out = {\"gates\": {\"safety_ok\": True, \"spec_ok\": True}, \"scores\": {}, \"reasons\": {\"parse_error\": text}}\n",
        "        final = BaseJudge.aggregate_scores(out.get(\"scores\", {}), out.get(\"gates\", {}))\n",
        "        out[\"final_score\"] = final\n",
        "        out[\"judge_backend\"] = \"anthropic\"\n",
        "        return out\n",
        "\n",
        "class LocalHFJudge(BaseJudge):\n",
        "    \"\"\"\n",
        "    Use a local open-source model as judge (e.g., Qwen2.5-3B-Instruct).\n",
        "    Requires transformers + bitsandbytes; not suitable for CPU-only.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_id: str = \"Qwen/Qwen2.5-3B-Instruct\"):\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "        import torch\n",
        "        bnb_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_cfg, device_map=\"auto\")\n",
        "        self.model.eval()\n",
        "\n",
        "    def _prompt(self, brief: Brief, cand: Candidate) -> str:\n",
        "        system = \"You are an expert brand evaluator. Return only JSON with scores 0..1 and one-sentence reasons.\"\n",
        "        user = f\"\"\"\n",
        "[BRIEF]\n",
        "title: {brief.title}\n",
        "language: {brief.language}\n",
        "tone: {brief.tone}\n",
        "keywords: {\", \".join(brief.keywords)}\n",
        "constraints: {brief.constraints}\n",
        "\n",
        "[CANDIDATE]\n",
        "domain: {cand.domain}\n",
        "rationale: {cand.rationale}\n",
        "\"\"\"\n",
        "        return f\"<|im_start|>system\\n{system}\\n<|im_end|>\\n<|im_start|>user\\n{user}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    def score_candidate(self, brief: Brief, cand: Candidate) -> Dict[str, Any]:\n",
        "        import torch, json as _json\n",
        "        text = self._prompt(brief, cand)\n",
        "        inputs = self.tok(text, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            out_ids = self.model.generate(**inputs, max_new_tokens=320, do_sample=False, temperature=0.0)\n",
        "        out = self.tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "        try:\n",
        "            data = _json.loads(out)\n",
        "        except Exception:\n",
        "            data = {\"gates\": {\"safety_ok\": True, \"spec_ok\": True}, \"scores\": {}, \"reasons\": {\"parse_error\": out}}\n",
        "        final = BaseJudge.aggregate_scores(data.get(\"scores\", {}), data.get(\"gates\", {}))\n",
        "        data[\"final_score\"] = final\n",
        "        data[\"judge_backend\"] = \"local_hf\"\n",
        "        return data\n",
        "\n",
        "# --------------------\n",
        "# Evaluation driver\n",
        "# --------------------\n",
        "\n",
        "def aggregate_report(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    finals = [r[\"final_score\"] for r in rows]\n",
        "    by_gate_fail = {\n",
        "        \"safety_fail\": sum(1 for r in rows if not r[\"gates\"][\"safety_ok\"]),\n",
        "        \"spec_fail\": sum(1 for r in rows if not r[\"gates\"][\"spec_ok\"]),\n",
        "    }\n",
        "    summary = {\n",
        "        \"count\": len(rows),\n",
        "        \"final_score_mean\": round(float(statistics.mean(finals)), 4) if finals else 0.0,\n",
        "        \"final_score_median\": round(float(statistics.median(finals)), 4) if finals else 0.0,\n",
        "        \"final_score_p90\": round(float(sorted(finals)[int(0.9*(len(finals)-1))]), 4) if finals else 0.0,\n",
        "        **by_gate_fail\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def make_brief(obj: Dict[str, Any]) -> Brief:\n",
        "    return Brief(\n",
        "        brief_id=obj[\"brief_id\"],\n",
        "        title=obj[\"title\"],\n",
        "        language=obj.get(\"language\",\"en\"),\n",
        "        tone=obj.get(\"tone\",\"\"),\n",
        "        keywords=obj.get(\"keywords\",[]),\n",
        "        constraints=obj.get(\"constraints\",{}),\n",
        "        notes=obj.get(\"notes\",\"\")\n",
        "    )\n",
        "\n",
        "def make_candidate(obj: Dict[str, Any]) -> Candidate:\n",
        "    return Candidate(\n",
        "        candidate_id=obj[\"candidate_id\"],\n",
        "        brief_id=obj[\"brief_id\"],\n",
        "        domain=obj[\"domain\"],\n",
        "        rationale=obj.get(\"rationale\",\"\"),\n",
        "        passes_constraints=obj.get(\"passes_constraints\", True),\n",
        "        safety=obj.get(\"safety\", {\"flagged\": False, \"reasons\": []})\n",
        "    )\n",
        "'''\n",
        "with open(os.path.join(BASE_DIR, \"llm_judge.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(llm_judge_py)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) run_judge_demo.py (uses RuleBasedJudge now)\n",
        "# ------------------------------\n",
        "run_demo_py = r'''\n",
        "\"\"\"\n",
        "run_judge_demo.py — Demo runner for the LLM-as-a-Judge framework.\n",
        "\n",
        "Usage (Colab):\n",
        "    !python run_judge_demo.py --data_dir /content/data --out_dir /content/eval --backend rule\n",
        "Backends:\n",
        "    rule  -> RuleBasedJudge (offline)\n",
        "    openai-> OpenAIJudge (requires OPENAI_API_KEY)\n",
        "    anthropic -> AnthropicJudge (requires ANTHROPIC_API_KEY)\n",
        "    local -> LocalHFJudge (loads open-source model; needs GPU & internet to download)\n",
        "\"\"\"\n",
        "\n",
        "import os, json, argparse, uuid\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "from llm_judge import (\n",
        "    RuleBasedJudge, OpenAIJudge, AnthropicJudge, LocalHFJudge,\n",
        "    make_brief, make_candidate, aggregate_report, RUBRIC_WEIGHTS\n",
        ")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                yield json.loads(line)\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data_dir\", type=str, required=True, help=\"Folder containing domain_briefs.jsonl and domain_candidates.jsonl\")\n",
        "    ap.add_argument(\"--out_dir\", type=str, required=True)\n",
        "    ap.add_argument(\"--backend\", type=str, default=\"rule\", choices=[\"rule\",\"openai\",\"anthropic\",\"local\"])\n",
        "    ap.add_argument(\"--max_per_brief\", type=int, default=10)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    briefs = list(load_jsonl(os.path.join(args.data_dir, \"domain_briefs.jsonl\")))\n",
        "    cands  = list(load_jsonl(os.path.join(args.data_dir, \"domain_candidates.jsonl\")))\n",
        "\n",
        "    brief_map = {b[\"brief_id\"]: make_brief(b) for b in briefs}\n",
        "    by_brief = defaultdict(list)\n",
        "    for c in cands:\n",
        "        by_brief[c[\"brief_id\"]].append(make_candidate(c))\n",
        "\n",
        "    # Select judge\n",
        "    if args.backend == \"rule\":\n",
        "        judge = RuleBasedJudge()\n",
        "    elif args.backend == \"openai\":\n",
        "        judge = OpenAIJudge(model=os.getenv(\"OPENAI_MODEL\",\"gpt-4o-mini\"))\n",
        "    elif args.backend == \"anthropic\":\n",
        "        judge = AnthropicJudge(model=os.getenv(\"ANTHROPIC_MODEL\",\"claude-3-5-sonnet\"))\n",
        "    else:\n",
        "        judge = LocalHFJudge(model_id=os.getenv(\"LOCAL_JUDGE_MODEL\",\"Qwen/Qwen2.5-3B-Instruct\"))\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    report_path = os.path.join(args.out_dir, \"evaluation_report.jsonl\")\n",
        "    summary_path = os.path.join(args.out_dir, \"evaluation_summary.md\")\n",
        "\n",
        "    rows = []\n",
        "    for b in briefs:\n",
        "        brief = brief_map[b[\"brief_id\"]]\n",
        "        # take up to N candidates per brief (prioritize ones that passed constraints in dataset)\n",
        "        cand_list = sorted(by_brief[brief.brief_id], key=lambda x: (x.passes_constraints, not x.safety.get(\"flagged\", False)), reverse=True)[:args.max_per_brief]\n",
        "        for cand in cand_list:\n",
        "            scored = judge.score_candidate(brief, cand)\n",
        "            rows.append({\n",
        "                \"eval_id\": str(uuid.uuid4()),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
        "                \"brief_id\": brief.brief_id,\n",
        "                \"brief_title\": brief.title,\n",
        "                \"domain\": cand.domain,\n",
        "                \"scores\": scored[\"scores\"],\n",
        "                \"gates\": scored[\"gates\"],\n",
        "                \"final_score\": scored[\"final_score\"],\n",
        "                \"spec_reasons\": scored.get(\"spec_reasons\", []),\n",
        "                \"safety_reasons\": scored.get(\"safety_reasons\", []),\n",
        "                \"backend\": scored.get(\"judge_backend\", \"unknown\")\n",
        "            })\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r)+\"\\n\")\n",
        "\n",
        "    # Aggregate\n",
        "    overall = aggregate_report(rows)\n",
        "\n",
        "    # Per-brief top picks\n",
        "    top_by_brief = defaultdict(list)\n",
        "    for r in rows:\n",
        "        top_by_brief[r[\"brief_id\"]].append(r)\n",
        "    for k in top_by_brief:\n",
        "        top_by_brief[k].sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
        "\n",
        "    # Write human-readable summary\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"# Evaluation Summary\\n\\n\")\n",
        "        f.write(f\"Generated: {datetime.utcnow().isoformat()}Z\\n\\n\")\n",
        "        f.write(f\"**Backend:** `{args.backend}`  \\n\")\n",
        "        f.write(f\"**Rubric weights:** `{RUBRIC_WEIGHTS}`\\n\\n\")\n",
        "        f.write(f\"## Overall Metrics\\n\")\n",
        "        for k, v in overall.items():\n",
        "            f.write(f\"- **{k}**: {v}\\n\")\n",
        "        f.write(\"\\n## Top picks per brief\\n\")\n",
        "        for b in briefs:\n",
        "            arr = top_by_brief[b['brief_id']][:3]\n",
        "            f.write(f\"\\n### {b['title']}\\n\")\n",
        "            for r in arr:\n",
        "                f.write(f\"- `{r['domain']}` — score {r['final_score']:.3f} (gates: {r['gates']})\\n\")\n",
        "\n",
        "    print(f\"Wrote:\\n- {report_path}\\n- {summary_path}\")\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raise SystemExit(main())\n",
        "'''\n",
        "with open(os.path.join(BASE_DIR, \"run_judge_demo.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(run_demo_py)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Create a minimal demo run using the dataset from earlier step (if present)\n",
        "# ------------------------------\n",
        "DATA_DIR = \"/mnt/data/domain_dataset_v1\"\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"demo_eval\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Check dataset presence\n",
        "has_dataset = all(os.path.exists(os.path.join(DATA_DIR, fn)) for fn in [\"domain_briefs.jsonl\",\"domain_candidates.jsonl\"])\n",
        "\n",
        "demo_note = {}\n",
        "if has_dataset:\n",
        "    # Run the rule-based judge to produce a report and summary\n",
        "    import subprocess, sys, textwrap\n",
        "    cmd = [\"python\", os.path.join(BASE_DIR, \"run_judge_demo.py\"),\n",
        "           \"--data_dir\", DATA_DIR, \"--out_dir\", OUT_DIR, \"--backend\", \"rule\", \"--max_per_brief\", \"8\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "        demo_note[\"ran_demo\"] = True\n",
        "    except Exception as e:\n",
        "        demo_note[\"ran_demo\"] = False\n",
        "        demo_note[\"error\"] = str(e)\n",
        "else:\n",
        "    demo_note[\"ran_demo\"] = False\n",
        "    demo_note[\"error\"] = \"Dataset from Step 1 not found at /mnt/data/domain_dataset_v1\"\n",
        "\n",
        "# Show what we created\n",
        "files = {\n",
        "    \"module\": os.path.join(BASE_DIR, \"llm_judge.py\"),\n",
        "    \"runner\": os.path.join(BASE_DIR, \"run_judge_demo.py\"),\n",
        "    \"report\": os.path.join(OUT_DIR, \"evaluation_report.jsonl\"),\n",
        "    \"summary\": os.path.join(OUT_DIR, \"evaluation_summary.md\")\n",
        "}\n",
        "\n",
        "files, demo_note\n"
      ],
      "metadata": {
        "id": "4BodOCmmApLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}