# LoRA SFT defaults â€” good starting point for 3B
train:
  num_train_epochs: 2
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.0
  logging_steps: 10
  save_steps: 200
  max_steps: -1
  max_seq_length: 2048
  packing: true
  gradient_checkpointing: true
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  quantization: "bnb_4bit"  # set to null for full precision
